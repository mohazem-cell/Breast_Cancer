{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdMwoRo1NSLU",
        "outputId": "48297d03-e1f1-49fd-84fd-4ce908e5c0cc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ac84759"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data** **Loading**"
      ],
      "metadata": {
        "id": "vuNPLjsEwWhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/BCD_data.csv\")"
      ],
      "metadata": {
        "id": "XmbfzDD6OgC-",
        "outputId": "d7be61f4-e32a-4ec1-f324-07ab900f0df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/BCD_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4224186004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/BCD_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/BCD_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset shape:\", data.shape)"
      ],
      "metadata": {
        "id": "BeA9o0a0wnkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.describe())"
      ],
      "metadata": {
        "id": "qXB_JR9Lw1Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "y0bk3fkCxNtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Column names:\", data.columns.tolist()) # Use tolist() for better readability"
      ],
      "metadata": {
        "id": "MpeoLVFKxSF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Object type columns:\", data.select_dtypes(include=\"object\").columns.tolist()) # Use tolist() for better readability\n"
      ],
      "metadata": {
        "id": "RFxsluN_xZO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values:\\n\", data.isna().sum())"
      ],
      "metadata": {
        "id": "CB3cX8PrxbXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vwYET2wNSLd"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXjhn7S4NSLv"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns='Unnamed: 32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfiZ2D7NSLw"
      },
      "source": [
        "### Dealing with categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSIVzL4mNSLw"
      },
      "outputs": [],
      "source": [
        "data['diagnosis'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'diagnosis' to numerical using one-hot encoding.\n",
        "data = pd.get_dummies(data=data, columns=['diagnosis'], drop_first=True)"
      ],
      "metadata": {
        "id": "vN-TKFnEKLdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the encoded column for clarity\n",
        "data.rename(columns={'diagnosis_M': 'diagnosis'}, inplace=True)"
      ],
      "metadata": {
        "id": "pz3zUtNOKNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert boolean to int (get_dummies with drop_first=True returns boolean)\n",
        "data['diagnosis'] = data['diagnosis'].astype(int)"
      ],
      "metadata": {
        "id": "qAbXYcL-KQbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData after preprocessing:\")\n",
        "display(data.head(4))"
      ],
      "metadata": {
        "id": "TDMwU3EUJn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizations**"
      ],
      "metadata": {
        "id": "JZ_xozPvLEOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Countplot of the target variable"
      ],
      "metadata": {
        "id": "gnhfKy0kLI_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1gw-SGqNSL0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=data['diagnosis'], palette='Set2')\n",
        "plt.show()\n",
        "print(f\"Total Malignant (1) cases: {(data['diagnosis']==1).sum()}\")\n",
        "print(f\"Total Benign (0) cases: {(data['diagnosis']==0).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Correlation matrix and heatmap"
      ],
      "metadata": {
        "id": "1KAKPlyGLjit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_corr = data.drop(columns='id')\n",
        "data_corr.corrwith(data_corr['diagnosis']).plot.bar(\n",
        "    figsize=(20,10), title='Correlation with Diagnosis (1: Malignant)',\n",
        "    rot=45, grid=True)\n",
        "plt.ylabel('Correlation Coefficient') # Add y-axis label\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FRBuRKinLxzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the full correlation matrix\n",
        "corr = data_corr.corr()\n",
        "corr"
      ],
      "metadata": {
        "id": "L09qWnzyMDDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the correlation heatmap\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkVxD1OPMP_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Drop high multicollinearity"
      ],
      "metadata": {
        "id": "Fnh0bPg8m9rm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67f85749"
      },
      "source": [
        "# Calculate the absolute correlation matrix\n",
        "corr_matrix = data.drop(columns=['id', 'diagnosis']).corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.9 (you can adjust this threshold)\n",
        "to_drop_multicollinearity = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "\n",
        "print(\"Features to drop due to high multicollinearity:\")\n",
        "print(to_drop_multicollinearity)\n",
        "\n",
        "# Drop the identified features from the DataFrame\n",
        "data_dropped_multicollinearity = data.drop(columns=to_drop_multicollinearity)\n",
        "\n",
        "print(\"\\nShape of data after dropping highly correlated features:\")\n",
        "print(data_dropped_multicollinearity.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix of the data after dropping highly correlated features\n",
        "corr_matrix_dropped = data_dropped_multicollinearity.drop(columns='diagnosis').corr()\n",
        "\n",
        "# Plot the correlation heatmap of the remaining features\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(corr_matrix_dropped, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Features After Dropping Highly Correlated Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqYodwLXjVK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying ML Techniques**\n"
      ],
      "metadata": {
        "id": "06Mr4YYcmXHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train and test\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns=['diagnosis','id']) # Drop 'id' as it's not a feature\n",
        "y = data['diagnosis']"
      ],
      "metadata": {
        "id": "yAkd14J-Mce1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
        "\n",
        "print(\"\\nShape of training data:\", X_train.shape)\n",
        "print(\"Shape of testing data:\", X_test.shape)"
      ],
      "metadata": {
        "id": "ZAgsG79yMhar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Scaling**"
      ],
      "metadata": {
        "id": "nW4LCFs3MmHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize StandardScaler\n",
        "sc = StandardScaler()"
      ],
      "metadata": {
        "id": "6xh88SeUM2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit and transform the training data, transform the testing data\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "WktU7LWENBcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display scaled test data\n",
        "display(pd.DataFrame(X_test_scaled, columns=X_test.columns).head()) # Display scaled data as DataFrame with column names"
      ],
      "metadata": {
        "id": "P29_veGwNJGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "I5a61pvHNRpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LC = LogisticRegression(random_state=0)\n",
        "LC.fit(X_train_scaled, y_train) # Train on scaled data\n",
        "y_pred_lc = LC.predict(X_test_scaled) # Predict on scaled data"
      ],
      "metadata": {
        "id": "9lVvAq5HNmt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random forest**"
      ],
      "metadata": {
        "id": "MdSwRFodQPgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier(random_state=0)\n",
        "RF.fit(X_train_scaled, y_train) # Train on scaled data\n",
        "y_pred_rf = RF.predict(X_test_scaled) # Predict on scaled data"
      ],
      "metadata": {
        "id": "vu8DzS4dQgiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**"
      ],
      "metadata": {
        "id": "8PEAsFDZQlOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(random_state=0)\n",
        "svm_model.fit(X_train_scaled, y_train) # Train on scaled data\n",
        "y_pred_svm = svm_model.predict(X_test_scaled) # Predict on scaled data"
      ],
      "metadata": {
        "id": "zKF7NyMqQwsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN**"
      ],
      "metadata": {
        "id": "FvkpksAnQ2LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(X_train_scaled, y_train) # Train on scaled data\n",
        "y_pred_knn = knn_model.predict(X_test_scaled) # Predict on scaled data"
      ],
      "metadata": {
        "id": "JlF6Uxd7RAhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "-9Y5TAUuRhLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train) # Train on scaled data\n",
        "y_pred_nb = nb_model.predict(X_test_scaled) # Predict on scaled data"
      ],
      "metadata": {
        "id": "OGcX2h7URvEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "ZAvffjOxS-eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['Logistic Regression', 'Random Forest', 'SVM', 'KNN', 'Naive Bayes']\n",
        "predictions = [y_pred_lc, y_pred_rf, y_pred_svm, y_pred_knn, y_pred_nb]\n",
        "results_list = []\n",
        "\n",
        "for model_name, y_pred in zip(models, predictions):\n",
        "    acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "    f1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
        "    prec = precision_score(y_true=y_test, y_pred=y_pred)\n",
        "    rec = recall_score(y_true=y_test, y_pred=y_pred)\n",
        "    results_list.append([model_name, acc, f1, prec, rec])\n",
        "\n",
        "result_df = pd.DataFrame(results_list, columns=['Model','Accuracy','F1 score', 'Precision score','Recall score'])"
      ],
      "metadata": {
        "id": "jlZFspz2TH0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the performance metrics for all models\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "63oXoZcMyJjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display confusion matrix for Logistic Regression as an example\n",
        "print(\"\\nConfusion Matrix for Logistic Regression:\")\n",
        "cm_lc = confusion_matrix(y_true=y_test, y_pred=y_pred_lc)\n",
        "print(cm_lc)"
      ],
      "metadata": {
        "id": "BNRk13DyzWpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation for all models\n",
        "print(\"\\nCross Validation Results:\")\n",
        "for model_name, model in zip(models, [LC, RF, svm_model, knn_model, nb_model]):\n",
        "    accuracies = cross_val_score(estimator=model, X=X_train_scaled, y=y_train, cv=10) # Use scaled training data\n",
        "    print(f\"{model_name} - Accuracy : {accuracies.mean()*100:.2f}%\")\n",
        "    print(f\"{model_name} - Std Deviation : {accuracies.std()*100:.2f}%\")"
      ],
      "metadata": {
        "id": "WDZEtBZHzd24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "YDiAcxKqtdDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "parameters = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}"
      ],
      "metadata": {
        "id": "-K93RiUz0VMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomizedSearchCV for Logistic Regression\n",
        "random_search_lr = RandomizedSearchCV(estimator=LC, param_distributions=parameters, n_iter=10, scoring='roc_auc', n_jobs=-1, cv=10, verbose=3, random_state=0) # Add random_state for reproducibility"
      ],
      "metadata": {
        "id": "oKSSD0xN0mzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit RandomizedSearchCV on scaled training data\n",
        "random_search_lr.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "oDI8AEAQ0p6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest estimator from Randomized Search (Logistic Regression):\")\n",
        "display(random_search_lr.best_estimator_)"
      ],
      "metadata": {
        "id": "oPgJ-PuF0xyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest ROC AUC score from Randomized Search (Logistic Regression):\")\n",
        "print(random_search_lr.best_score_)"
      ],
      "metadata": {
        "id": "wSQbfZRp0z_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest parameters from Randomized Search (Logistic Regression):\")\n",
        "print(random_search_lr.best_params_)"
      ],
      "metadata": {
        "id": "exag3mzt03Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Model (Logistic Regression) - Train with best parameters**"
      ],
      "metadata": {
        "id": "3sacrI7y2PtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the final Logistic Regression model using the best parameters found\n",
        "LRF = LogisticRegression(C=random_search_lr.best_params_['C'],\n",
        "                           penalty=random_search_lr.best_params_['penalty'],\n",
        "                           solver=random_search_lr.best_params_['solver'],\n",
        "                           random_state=0)\n",
        "\n",
        "LRF.fit(X_train_scaled, y_train) # Train the final model on scaled data"
      ],
      "metadata": {
        "id": "aPWZOtUB2X-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the final model on the test set\n",
        "y_pred_lrf = LRF.predict(X_test_scaled)\n",
        "\n",
        "acc_lrf = accuracy_score(y_true=y_test, y_pred=y_pred_lrf)\n",
        "f1_lrf = f1_score(y_true=y_test, y_pred=y_pred_lrf)\n",
        "prec_lrf = precision_score(y_true=y_test, y_pred=y_pred_lrf)\n",
        "rec_lrf = recall_score(y_true=y_test, y_pred=y_pred_lrf)"
      ],
      "metadata": {
        "id": "W5dxQ5Kh2cxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the final model's results to the result DataFrame\n",
        "final_model_result = pd.DataFrame([[\"Final Model LR (Tuned)\", acc_lrf, f1_lrf, prec_lrf, rec_lrf]], columns=['Model','Accuracy','F1 score', 'Precision score','Recall score'])\n",
        "result_df = pd.concat([result_df, final_model_result], ignore_index=True)"
      ],
      "metadata": {
        "id": "wU0epSOc2hOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the updated result DataFrame including the tuned model\n",
        "display(result_df)"
      ],
      "metadata": {
        "id": "0pCqHGOs2kwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross validation**"
      ],
      "metadata": {
        "id": "2omUhx2K3xEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation for the final tuned Logistic Regression model\n",
        "print(\"\\nCross Validation Results for Final Tuned Logistic Regression Model:\")\n",
        "accuracies_lrf = cross_val_score(estimator=LRF, X=X_train_scaled, y=y_train, cv=10)\n",
        "print(\"Accuracy : \",accuracies_lrf.mean()*100,'%')\n",
        "print(\"Std Deviation : \",accuracies_lrf.std()*100,'%')"
      ],
      "metadata": {
        "id": "xGho0gk03k95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a9465b0"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top features using f-classif on scaled training data\n",
        "selector = SelectKBest(score_func=f_classif, k=10)  # You can change k to select a different number of features\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "maqKga934a_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the selected feature names from the original column names\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"\\nSelected features after applying SelectKBest on scaled data:\")\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "IG7hdfhq4c53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgH7dAthNSMp"
      },
      "source": [
        "### Single Observations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating prediction on a single observation using the final tuned Logistic Regression model\n",
        "single_obs = [[17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189]]\n"
      ],
      "metadata": {
        "id": "Su7DL_zT4pTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scale the single observation using the same scaler fitted on the training data\n",
        "single_obs_scaled = sc.transform(single_obs)"
      ],
      "metadata": {
        "id": "HTmw-0nN4scJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the final tuned Logistic Regression model\n",
        "predicted_diagnosis = LRF.predict(single_obs_scaled)"
      ],
      "metadata": {
        "id": "ZiA5efg-5HBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interpret the prediction**\n"
      ],
      "metadata": {
        "id": "2T678t1OTB05"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weU_kfSDuwnS"
      },
      "source": [
        "if predicted_diagnosis[0] == 1:\n",
        "    print(\"\\nPrediction for the single observation: Malignant (1)\")\n",
        "else:\n",
        "    print(\"\\nPrediction for the single observation: Benign (0)\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}